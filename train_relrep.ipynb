{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nisal.20/fyp/LangBridge/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/home/nisal.20/fyp/LangBridge/.venv/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:515: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4add033c8014d1c8f29fab5f37ea6ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nisal.20/fyp/LangBridge/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:433: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mOutput I:\u001b[0m\n",
      "\n",
      "\n",
      "#### 16\n",
      "The answer is:\n",
      "\u001b[91mOutput II:\u001b[0m\n",
      "\n",
      "A bat has 2 rolls of blue tape and half that amount of white tape.\n",
      "So, the bat has 2/2 = 1 roll of white tape.\n",
      "In total, the bat needs 2 rolls of blue tape + 1 roll of white tape = 3 rolls of tape.\n",
      "#### 3\n",
      "The answer is: 3\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from langbridge import LangBridgeModel\n",
    "\n",
    "enc_tokenizer = AutoTokenizer.from_pretrained('DKYoon/mt5-xl-lm-adapt') \n",
    "lm_tokenizer = AutoTokenizer.from_pretrained('meta-math/MetaMath-7B-V1.0')\n",
    "model = LangBridgeModel.from_pretrained('python_scripts/checkpoints/MetaMath-7B-V1.0-linear/epoch=1-step=1562').to('cuda')\n",
    "orca_template = \"<|im_start|>system\\n{system_message}<|im_end|>\\n<|im_start|>user\\n{user_message}<|im_end|>\\n<|im_start|>assistant\"\n",
    "\n",
    "system_message1 = \"You are a knowledgeable AI assistant. When given a question, provide a comprehensive and detailed response that thoroughly explains all calculations, logic, and steps involved. Your answers should be long, insightful, and leave no detail unexplored, aiming to provide the user with a complete understanding of the topic.\" \n",
    "user_message1 = \"Los patos de Janet ponen 16 huevos por día. Ella come tres en el desayuno todas las mañanas y usa cuatro para hornear magdalenas para sus amigos todos los días. Vende lo que sobra en el mercado de productores diariamente a $2 el huevo fresco de pato. ¿Cuánto gana en dólares todos los días en el mercado de productores?\"\n",
    "\n",
    "system_message2 = \"You are a friendly and helpful assistant who explains things in a simple and clear way. When answering questions, imagine you are talking to a five-year-old child. Use easy words, short sentences, and provide examples that are easy to understand. Your goal is to make everything as clear and friendly as possible, so even a young child can follow along.\"\n",
    "user_message2 = \"Una bata lleva 2 rollos de tela azul y la mitad de esa cantidad de tela blanca. ¿Cuántos rollos se necesitan en total?\" # Explain why GPT-3 is so influential.\n",
    "\n",
    "\n",
    "prompt1 = orca_template.format(system_message=system_message1, user_message=user_message1)\n",
    "prompt2 = orca_template.format(system_message=system_message2, user_message=user_message2)\n",
    "\n",
    "output = model.generate_from_prefix(enc_tokenizer, lm_tokenizer, prompts=[prompt1, prompt2], max_length=300)\n",
    "print(\"\\033[91mOutput I:\\033[0m\")\n",
    "print(output[0])\n",
    "print(\"\\033[91mOutput II:\\033[0m\")\n",
    "print(output[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nisal.20/fyp/LangBridge/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/nisal.20/fyp/LangBridge/.venv/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:515: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1fc261c4b31469d84906a1ce18ad02e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nisal.20/fyp/LangBridge/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:433: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mOutput I:\u001b[0m\n",
      "The pats of Janet put 16 huevos per day.\n",
      "She eats 3 huevos in the morning every day\n",
      "She uses 4 huevos to make magdalenas every day\n",
      "So, she uses a total of 3 + 4 = 7 huevos per day\n",
      "She sells the remaining huevos in the market for $2 each\n",
      "Therefore, she earns 7 * $2 = $14 per day in the market\n",
      "#### 14\n",
      "The answer is: 14\n",
      "\u001b[91mOutput II:\u001b[0m\n",
      "The number of rolls of blue tape is 2.Љ The number of rolls of white tape is 1/2 * 2 = 1 roll.Љ In total, you need 2 + 1 = 3 rolls of tape.\n",
      "#### 3\n",
      "The answer is: 3\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from langbridge import LangBridgeModel\n",
    "\n",
    "enc_tokenizer = AutoTokenizer.from_pretrained('DKYoon/mt5-xl-lm-adapt') \n",
    "lm_tokenizer = AutoTokenizer.from_pretrained('meta-math/MetaMath-7B-V1.0')\n",
    "model = LangBridgeModel.from_pretrained('python_scripts/checkpoints/MetaMath-7B-V1.0-rr-4096/epoch=1-step=1562').to('cuda')\n",
    "orca_template = \"<|im_start|>system\\n{system_message}<|im_end|>\\n<|im_start|>user\\n{user_message}<|im_end|>\\n<|im_start|>assistant\"\n",
    "\n",
    "system_message1 = \"You are an AI assistant. You will be given a task. You must generate a detailed and long answer.\" \n",
    "user_message1 = \"Los patos de Janet ponen 16 huevos por día. Ella come tres en el desayuno todas las mañanas y usa cuatro para hornear magdalenas para sus amigos todos los días. Vende lo que sobra en el mercado de productores diariamente a $2 el huevo fresco de pato. ¿Cuánto gana en dólares todos los días en el mercado de productores?\"\n",
    "\n",
    "system_message2 = \"You are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.\"\n",
    "user_message2 = \"Una bata lleva 2 rollos de tela azul y la mitad de esa cantidad de tela blanca. ¿Cuántos rollos se necesitan en total?\" # Explain why GPT-3 is so influential.\n",
    "\n",
    "\n",
    "prompt1 = orca_template.format(system_message=system_message1, user_message=user_message1)\n",
    "prompt2 = orca_template.format(system_message=system_message2, user_message=user_message2)\n",
    "\n",
    "output = model.generate_from_prefix(enc_tokenizer, lm_tokenizer, prompts=[prompt1, prompt2], max_length=300)\n",
    "print(\"\\033[91mOutput I:\\033[0m\")\n",
    "print(output[0])\n",
    "print(\"\\033[91mOutput II:\\033[0m\")\n",
    "print(output[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
